{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n    \n    return df\n\n\ndef import_data(file):\n    \"\"\"create a dataframe and optimize its memory usage\"\"\"\n    df = pd.read_csv(file, parse_dates=True, keep_date_col=True)\n    df = reduce_mem_usage(df)\n    return df","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Import modules\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport math\n\nfrom sklearn.model_selection import train_test_split\n\n#keras\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import BatchNormalization, Dense, Dropout, Flatten\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, AveragePooling2D\nfrom tensorflow.keras.callbacks import LearningRateScheduler, EarlyStopping\nimport sklearn.metrics as metrics\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = import_data(r'../input/emnist/emnist-bymerge-train.csv')\ntest = import_data(r'../input/emnist/emnist-bymerge-test.csv')\n\n#mapp = pd.read_csv(\n #   r'../input/emnist/emnist-bymerge-mapping.txt',\n #   delimiter=' ',\n #   index_col=0,\n #   header=None,\n #   squeeze=True\n#)\n\nprint(\"Train: %s, Test: %s\" %(train.shape, test.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Constants\nHEIGHT = 28\nWIDTH = 28","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Split x and y\ntrain_x = train.iloc[:,1:] # Get the images\ntrain_y = train.iloc[:,0] # Get the label\ndel train # free up some memory\n\ntest_x = test.iloc[:,1:]\ntest_y = test.iloc[:,0]\ndel test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Reshape and rotate EMNIST images\ndef rotate(image):\n    image = image.reshape(HEIGHT, WIDTH)\n    image = np.fliplr(image)\n    image = np.rot90(image)\n    return image ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Flip and rotate image\ntrain_x = np.asarray(train_x)\ntrain_x = np.apply_along_axis(rotate, 1, train_x)\nprint (\"train_x:\",train_x.shape)\n\ntest_x = np.asarray(test_x)\ntest_x = np.apply_along_axis(rotate, 1, test_x)\nprint (\"test_x:\",test_x.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Normalize\ntrain_x = train_x / 255.0\ntest_x = test_x / 255.0\nprint(type(train_x[0,0,0]))\nprint(type(test_x[0,0,0]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Plot image\n'''\nfor i in range(100,109):\n  plt.subplot(330 + (i+1))\n  plt.subplots_adjust(hspace=0.5, top=1)\n  plt.imshow(train_x[i], cmap=plt.get_cmap('gray'))\n  plt.title(chr(mapp[train_y[i]]))\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Number of classes\nnum_classes = train_y.nunique() # .nunique() returns the number of unique objects\nprint(num_classes) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# One hot encoding\ntrain_y = to_categorical(train_y, num_classes)\ntest_y = to_categorical(test_y, num_classes)\nprint(\"train_y: \", train_y.shape)\nprint(\"test_y: \", test_y.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# partition to train and val\ntrain_x, val_x, train_y, val_y = train_test_split(train_x, \n                                                  train_y, \n                                                  test_size=0.10, \n                                                  random_state=7)\n\nprint(train_x.shape, val_x.shape, train_y.shape, val_y.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Reshape\ntrain_x = train_x.reshape(-1, HEIGHT, WIDTH, 1)\ntest_x = test_x.reshape(-1, HEIGHT, WIDTH, 1)\nval_x = val_x.reshape(-1, HEIGHT, WIDTH, 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create more images via data augmentation\ndatagen = ImageDataGenerator(\n    rotation_range = 10,\n    zoom_range = 0.10,\n    width_shift_range=0.1,\n    height_shift_range=0.1\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Building model\n# ((Si - Fi + 2P)/S) + 1\n\nmodel = Sequential()\n\nmodel.add(Conv2D(32, kernel_size=3, \n                 activation='relu', input_shape=(HEIGHT, WIDTH, 1)))\n#model.add(AveragePooling2D(pool_size=(2,2), strides=(2,2)))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(32, kernel_size=3,activation='relu'))\n#model.add(AveragePooling2D(pool_size=(2,2)))\nmodel.add(BatchNormalization())\n#model.add(MaxPooling2D(pool_size=(2,2)))\nmodel.add(Conv2D(32, kernel_size=5, strides=2, padding='same', activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.4))\n\nmodel.add(Conv2D(64, kernel_size=3, activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(64, kernel_size=3, activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(64, kernel_size=5, strides=2, padding='same', activation='relu'))\n#model.add(MaxPooling2D(pool_size=(2,2)))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.4))\n\nmodel.add(Conv2D(128, kernel_size=4, activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Flatten())\nmodel.add(Dropout(0.4))\nmodel.add(Dense(units=num_classes, activation='softmax'))\n\ninput_shape = (None, HEIGHT, WIDTH, 1)\nmodel.build(input_shape)\nmodel.summary()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"my_callbacks = [\n    # Decrease learning rate\n    LearningRateScheduler(lambda x: 1e-3 * 0.95 ** x),\n    # Training will stop there is no improvement in val_loss after 3 epochs\n    EarlyStopping(monitor=\"val_acc\", \n                  patience=3, \n                  mode='max', \n                  restore_best_weights=True)\n]\n\n# TRAIN NETWORKS\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nhistory = model.fit(train_x, train_y, \n                    epochs=40,\n                    verbose=1, validation_data=(val_x, val_y), \n                    callbacks=my_callbacks)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot accuracy and loss\n'''\ndef plotgraph(epochs, acc, val_acc):\n    # Plot training & validation accuracy values\n    plt.plot(epochs, acc, 'b')\n    plt.plot(epochs, val_acc, 'r')\n    plt.title('Model accuracy')\n    plt.ylabel('Accuracy')\n    plt.xlabel('Epoch')\n    plt.legend(['Train', 'Val'], loc='upper left')\n    plt.show()\n'''\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#%%\n'''\nacc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(1,len(acc)+1)\n'''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Accuracy curve\n#plotgraph(epochs, acc, val_acc)\n\n# loss curve\n#plotgraph(epochs, loss, val_loss)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"score = model.evaluate(test_x, test_y, verbose=0)\nprint(\"Test loss:\", score[0])\nprint(\"Test accuracy:\", score[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.save(\"emnist_model.h5\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.save_weights(\"emnist_model_weights.h5\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = model.predict(test_x)\ny_pred = (y_pred > 0.5)\n\ncm = metrics.confusion_matrix(test_y.argmax(axis=1), y_pred.argmax(axis=1))\nprint(cm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}